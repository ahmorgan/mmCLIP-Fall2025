# Examining Signal-Text Contrastive Alignment for Zero-shot mmWave-based Human Activity Recognition

#### Andrew Morgan
#### Advised by Dr. Hongfei Xue



Abstract
Human activity recognition (HAR) is the study of intelligent systems that can understand and identify human activities. Recent HAR research has taken advantage of advances in deep learning to robustly learn and classify many complex activities; it has also seen a rise in the use of mmWave radar signals for activity classification, noted among IoT sensing technologies for their non-invasiveness and ubiquity. However, despite the ubiquity of mmWave, the collection of large amounts of mmWave signal training data often required for deep learning is impractical and laborious. To alleviate this, recent work has proposed the use of synthetic mmWave signal data generated from large labeled datasets of motion capture footage of human activities, such as BABEL (Punnakkal, et al.). In particular, mmCLIP (Cao, et al.), inspired by OpenAIâ€™s landmark CLIP model (Radford, et al.), proposed contrastively aligning synthetic mmWave signals and LLM-generated descriptions of text activity labels into a shared embedding space such that unseen signals can be easily associated with an activity label at inference, enabling zero-shot HAR. While mmCLIP demonstrates the potential of such an approach, it only incorporates a single instance-level contrastive loss between signal and text embeddings, which possibly leads to suboptimal contrastive alignment and zero-shot performance. Recent works such as MGCA (Wang, et al.) have proposed alternative multimodal contrastive objectives that allow models to learn more robustly aligned embeddings with little additional cost. We investigate alternative training objectives for boosting contrastive learning, and apply them to mmCLIP for zero-shot HAR. In particular, we present a category-level objective that allows the model to more robustly learn broad features. We also investigate use of the Muon gradient descent optimizer to more quickly pretrain and finetune our model while maintaining performance.
